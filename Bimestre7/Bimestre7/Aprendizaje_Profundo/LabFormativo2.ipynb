{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab formativo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En este laboratorio vamos entrenar un modelo para predicir la temperatura media dado un año, mes y ciudad\n",
    "\n",
    "#### Dado las restricciones de poder de computo de Coursera, les recomendo descargar el notebook y ejecutar en vuestras computadoras o en google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar al ejecutar en vuestros computadoras\n",
    "# try:\n",
    "#     import pandas as pd\n",
    "#     import numpy as np\n",
    "#     import torch\n",
    "#     import torch.nn as nn\n",
    "#     import torch.optim as optim\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# except ModuloNotFoundError as err:\n",
    "#     !pip install pandas numpy torch scikit-learn\n",
    "#     import pandas as pd\n",
    "#     import numpy as np\n",
    "#     import torch\n",
    "#     import torch.nn as nn\n",
    "#     import torch.optim as optim\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'GlobalLandTemperaturesByCity.csv' # Está disponible en el Coursera, para que puedan trabajar desde sus computadoras.\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path)\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "country_count = df['Country'].value_counts()\n",
    "country_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para que sea más plausible entrenar nuestro modelo, elegiremos un país del conjunto de datos. Si observamos cómo están distribuidos los datos, podemos notar que están desbalanceados, lo que requeriría mucho más trabajo de manejo de datos.\n",
    "\n",
    "En casos como este, generalmente tendríamos que aplicar técnicas de aumento de datos, como hicimos con el conjunto de datos Iris Flower, o tal vez hacer combinaciones con otros conjuntos de datos eliminando duplicados y obteniendo más ejemplos. También es posible reducir la cantidad de ejemplos en el conjunto de datos, pero no es suficiente simplemente mantener la misma cantidad de datos de manera arbitraria; debemos verificar si son de fechas similares, etc.\n",
    "\n",
    "Para evitar todo ese trabajo, trabajaremos solo con el país que tiene más ejemplos, en este caso, India."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Country'].isin(['India'])]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para que nuestro modelo pueda entender mejor los datos, normalizaremos la temperatura. Esto nos ayudará a obtener mejores resultados y a que el algoritmo calcule más rápido. También cambiaremos el formato de la fecha al formato mes año como columnas a parte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df['AverageTemperature'] = scaler.fit_transform(df[['AverageTemperature']])\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "df['City'] = labelencoder.fit_transform(df['City']) # Acá vamos crear mapeos para las ciudades, ejemplo ficticio Abohar = 0 ... Yelahanka = 35\n",
    "\n",
    "\n",
    "df['Year'] = pd.DatetimeIndex(df['dt']).year\n",
    "\n",
    "df = df[df['Year'] >= 1980] # Usaremos solo los datos posteriores al año 1980, esto por limitación de Ram de Coursera y para que el modelo termine de entrenar.\n",
    "\n",
    "df['Month'] = pd.DatetimeIndex(df['dt']).month\n",
    "\n",
    "print(\"\\nTransformed DataFrame:\")\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquí proporciono una versión condensada del conjunto de datos para agilizar la inferencia y el entrenamiento. Utilice exclusivamente esta versión para determinar qué hiperparámetros y configuraciones emplear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nTransformed DataFrame:\")\n",
    "df_small = df.sample( df.size //100, replace=True)\n",
    "df_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí definimos nuestro modelo. Cabe destacar que la configuración exacta de capas y neuronas no es obligatoria; siéntanse libres de modificar estos ajustes si creen que se pueden hacer mejoras.\n",
    "\n",
    "Verán que estamos aplicando los conceptos aprendidos esta semana, incluyendo la normalización y el dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TemperaturePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(2, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc4 = nn.Linear(256, 256)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc5 = nn.Linear(256, 1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.tanh(self.fc3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.tanh(self.fc4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí definimos cuáles serán los datos de entrada y qué es lo que vamos a predecir. Además, para un mejor manejo de los datos, utilizaremos un DataLoader. Esta función nos permite entrenar en pequeños lotes, lo que facilita una convergencia del modelo más rápida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "df_train = df[[\"Year\", \"Month\"]]\n",
    "df_target = df[\"AverageTemperature\"]\n",
    "\n",
    "X = torch.tensor(df_train.values, dtype=torch.float32)\n",
    "y = torch.tensor(df_target.values, dtype=torch.float32).view(-1,1)\n",
    "\n",
    "indices = torch.randperm(len(X))\n",
    "\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.02, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False) \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 1 DataLoader para el entrenamiento y un para la validación.\n",
    "\n",
    "Abajo procedemos a entrenar el modelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = TemperaturePredictor().to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.7)\n",
    "loss_values = []\n",
    "accumulation_steps = 4\n",
    "for epoch in range(2):\n",
    "    for i, (batch_X, batch_y) in enumerate(train_loader):\n",
    "        # Move data to GPU\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss = loss / accumulation_steps  # Normalize loss\n",
    "        loss.backward()\n",
    "        loss_values.append(loss.item())\n",
    "        # Update weights\n",
    "        if (i+1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(loss.item())\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss activacion: {loss.item()} \")\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim procedemos a validar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "total_val_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in val_loader:\n",
    "        # Forward pass\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "average_val_loss = total_val_loss / len(val_loader)\n",
    "print(f\"Validation Loss: {average_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_regression_model_with_loader(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []  # To store true labels\n",
    "    right = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_val_loss += loss.item() * len(batch_y)\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            true_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    average_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "    \n",
    "    # Check the number of predictions within a certain tolerance level\n",
    "    for i in range(len(predictions)):\n",
    "        if true_labels[i] - 0.12 <= predictions[i] <= true_labels[i] + 0.12:\n",
    "            right += 1\n",
    "\n",
    "    accuracy_within_tolerance = right / len(predictions)\n",
    "    #print(list(zip(predictions, true_labels)))\n",
    "    print(list(zip(predictions[0], true_labels[0])))\n",
    "    return average_val_loss, accuracy_within_tolerance\n",
    "\n",
    "# Dummy criterion and device for demonstration\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "device='cpu'\n",
    "# Run the validation loop\n",
    "average_val_loss, accuracy_within_tolerance = validate_regression_model_with_loader(model, val_loader, criterion, device)\n",
    "print(f\"Average Validation Loss: {average_val_loss}\")\n",
    "print(f\"Accuracy within Tolerance: {accuracy_within_tolerance * 100}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cambia mi resultado si cambio el tamaño del lote (batch size)?\n",
    "## ¿Y la tasa de aprendizaje (Learning Rate)?\n",
    "## ¿Qué sucede si elimino las capas de normalización por lotes (Batch Normalization), reduzco el tamaño del lote y aumento la cantidad de épocas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
