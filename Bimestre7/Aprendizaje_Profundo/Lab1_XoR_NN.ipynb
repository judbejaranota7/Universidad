{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d107ba9fe62584b40988708c12e291e",
     "grade": false,
     "grade_id": "cell-0dc11144a8e1d742",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Neural Network for XoR function in Python\n",
    "## En el tutorial 2 habimos creado una pequeña red que intentaba aprender la función XoR, pero nos dimos cuenta que mismo despues de 10.000 iteraciones nuestro error no lograba disminuir, incluso aumentaba.\n",
    "### En este laboratorio iremos a jugar con las posibles causas para poder descobrir como solucionar el problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "452cc61d2e0b30f04654866dd650d5c6",
     "grade": false,
     "grade_id": "cell-707aff07600b4290",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Empecemos entendendo el código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Sigmoid:\n",
    "    def derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "sigmoid = Sigmoid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81e5cf1cac8fa8d43da95b6fe80ea948",
     "grade": false,
     "grade_id": "cell-80b246466b707bf2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### En la celda de arriba tenemos nuestra función de activación y su derivada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización aleatoria con valores entre -1 y 1\n",
    "w11, w21, w12, w22, w_n1, w_n2 = 2*np.random.random() - 1, 2*np.random.random() - 1, 2*np.random.random() - 1, 2*np.random.random() - 1, 2*np.random.random() - 1, 2*np.random.random() - 1\n",
    "b1, b2, b3 = 2*np.random.random() - 1, 2*np.random.random() - 1, 2*np.random.random() - 1\n",
    "\n",
    "# Datos XOR\n",
    "entradas = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "salidas_deseadas = np.array([[0], [1], [1], [0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b3922e41c2c07e363de487a8cd9ff02",
     "grade": false,
     "grade_id": "cell-5ab14e8a8571a4fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Acá estavamos inicializando los pesos y sesgos, además de definir nuestro valores de entrar, empezemos haciendo una pequeña modificación, para que nuestro código pueda ser reutilizable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weights, bias, activation = sigmoid):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.output = 0\n",
    "        self.inputs = []\n",
    "        self.error = 0\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        self.output = self.activation(total)\n",
    "        return self.output\n",
    "    \n",
    "    def backpropagate_error(self, error):\n",
    "        self.error = self.activation.derivative(self.output) * error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "869272c72fad6455cbd523f279a8c258",
     "grade": false,
     "grade_id": "cell-5e84b3745d00677d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ahora tenemos una clase Nuerona, esta nos va servir para guardar la información de su valores de entrada, salida y además\n",
    "### va facilitar la propagación de error, tema que no habiamos visto en la tutoria 2\n",
    "\n",
    "### Tome su tiempo para leer y entender el código "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_red, output, h_layers, learning_rate):\n",
    "        self.learning_rate = 0.1\n",
    "        self.layers = []\n",
    "        self.input = input_red\n",
    "        self.__init__layers(h_layers, output)\n",
    "    \n",
    "    def __init__layers(self, h_layers, output):\n",
    "        for indice_layer in range(len(h_layers)):\n",
    "            \n",
    "                layer = []\n",
    "                for cantidad_de_neuronas in range(h_layers[indice_layer]):\n",
    "                    if indice_layer == 0:\n",
    "                        neurona = Neuron(\n",
    "                            [random.uniform(-1, 1) for _ in range(self.input)], random.uniform(-1, 1)\n",
    "                        )\n",
    "                    else:\n",
    "                        neurona = Neuron(\n",
    "                            [random.uniform(-1, 1) for _ in range(h_layers[indice_layer-1])], random.uniform(-1, 1)\n",
    "                        )\n",
    "                    layer.append(neurona)\n",
    "                self.layers.append(layer)\n",
    "                \n",
    "        self.layers.append([Neuron([random.uniform(-1, 1) for _ in range(h_layers[-1])], random.uniform(-1, 1), sigmoid) for _ in range(output)])\n",
    "    \n",
    "    \n",
    "    def feedforward(self, inputs):\n",
    "        for indice_capas in range(len(self.layers)):\n",
    "            outputs = []\n",
    "            for indice_neurona in range(len(self.layers[indice_capas])):\n",
    "                outputs.append(self.layers[indice_capas][indice_neurona].feedforward(inputs))\n",
    "            inputs = outputs # para la proxima capa recibir lo que retorna la capa que acabamos de pasar\n",
    "        return outputs\n",
    "    \n",
    "    # Acá era nuestro ciclo for para poder entrenar, pero encapsulado en un método\n",
    "    def train(self, inputs, targets):\n",
    "        outputs = self.feedforward(inputs)\n",
    "        errors = [targets[i] - outputs[i] for i in range(len(outputs))]\n",
    "    \n",
    "        for indice_layers in range(len(self.layers)-1, -1, -1):\n",
    "            layer = self.layers[indice_layers]\n",
    "            next_layer = self.layers[indice_layers+1] if indice_layers != len(self.layers)-1 else None\n",
    "            for indice_neurona in range(len(layer)):\n",
    "                neuron = layer[indice_neurona]\n",
    "                if next_layer is None:\n",
    "                    neuron.backpropagate_error(errors[indice_neurona])\n",
    "                else:\n",
    "                    error = 0\n",
    "                    for n in next_layer:\n",
    "                        error += n.error * n.weights[indice_neurona]\n",
    "                    neuron.backpropagate_error(error)    \n",
    "\n",
    "        for indice_layers in range(len(self.layers)):\n",
    "            for indice_neurona in range(len(self.layers[indice_layers])):\n",
    "                \n",
    "                neuron = self.layers[indice_layers][indice_neurona]\n",
    "            \n",
    "                for indice_peso in range(len(neuron.weights)):\n",
    "                    neuron.weights[indice_peso] += self.learning_rate * neuron.error * neuron.inputs[indice_peso]\n",
    "                neuron.bias += self.learning_rate * neuron.error\n",
    "\n",
    "        error_final = sum([0.5*e**2 for e in errors]) # 1/2 Sum (i=0, n) [ y- ^y]**2\n",
    "        return error_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f74cb5698e43b91f9404f4b7559b496",
     "grade": false,
     "grade_id": "cell-c2dac07e1027455e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Apesar del código ser mucho más largo, hace exactamente el mismo proceso que hemos estado haciendo, con la pequeña diferencia que estamos usando el algoritimo de backpropagate para poder calcular las derivadas de forma optima, de esta forma puedemos tener capas ocultas con varias y varias neuronas, y no tendremos que estar a mano calculando sus derivadas, el programa va hacer por nosotros =D\n",
    "\n",
    "### Si antes ya era importante entender el código, ahora es más aún, así que leean con calma =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Error: 0.5082580449367712\n",
      "Epoch 1000 Error: 0.5037089903052132\n",
      "Epoch 2000 Error: 0.4993695034779529\n",
      "Epoch 3000 Error: 0.469129955954955\n",
      "Epoch 4000 Error: 0.39463806377325583\n",
      "Epoch 5000 Error: 0.33631166152091574\n",
      "Epoch 6000 Error: 0.08277877624222173\n",
      "Epoch 7000 Error: 0.026686073664480583\n",
      "Epoch 8000 Error: 0.014792412123238586\n",
      "Epoch 9000 Error: 0.010014947376554995\n",
      "Input: [0, 0], Output: [0.06759003158412805]\n",
      "Input: [0, 1], Output: [0.9449799969425877]\n",
      "Input: [1, 0], Output: [0.9384504161834119]\n",
      "Input: [1, 1], Output: [0.060002792335910424]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(2, 1, [2], learning_rate=0.1000)\n",
    "#  input_red = 2, output = 1, h_layers= 1: misma configuración que teniamos antes\n",
    "# h_layers = hidden layers o capas ocultas en español\n",
    "    # XOR Input and Output\n",
    "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "Y = [[0], [1], [1], [0]]\n",
    "\n",
    "# Training\n",
    "for epoca in range(10000):\n",
    "    error = 0\n",
    "    for indice_error in range(len(X)):\n",
    "        error += nn.train(X[indice_error], Y[indice_error])\n",
    "    if epoca % 1000 == 0:\n",
    "        print(f\"Epoch {epoca} Error: {error}\")\n",
    "\n",
    "# Testing\n",
    "for x in X:\n",
    "    print(f\"Input: {x}, Output: {nn.feedforward(x)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec87da863ce7440e9c6decfb33775045",
     "grade": false,
     "grade_id": "cell-5b10f3e8f8ed0ad5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Como ya habiamos visto en este ejemplo, nuestro error esta bien alto y no logra disminuir de 0.49, porque no estamos podiendo aprender la representación de la función XoR?\n",
    "- Podemos cambiar algunos parametros para ver si el problema es nuestro learning rate? Sera la función de activación? O sera que la cantidad de neuronas que tenemos es demasiado pequeña para poder representar la función XoR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c877901924d2a760a265931cc3da114",
     "grade": false,
     "grade_id": "cell-fddb52a52d298235",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# El objetivo es que el lector, consiga solucionar el problema, así que pasos a intentar\n",
    "- aumentar learnig rate y ver si cambia el resultado\n",
    "- aumentar el número de capas ocultas\n",
    "- aumentar el número de epocas\n",
    "- volver a disminuir el learning rate y dejar las capas ocultas en 2, que pasa ahora?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
